# Audio Deepfake Detection Eval

<p align="center">
    <br>
    <img src="docs/en/_static/images/add_eval_logo.png"/>
    <br>
</p>

<p align="center">
  <a href="README_zh.md">ä¸­æ–‡</a> &nbsp ï½œ &nbsp English &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg">
<a href="https://github.com/ntu-dtc/audio_deepfake_eval/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
</p>

A comprehensive evaluation framework for audio deepfake detection (ADD) from [NTU DTC](https://www.ntu.edu.sg/dtc), providing standardized tools for computing Equal Error Rates (EERs) and analyzing model performance across diverse datasets.

## ğŸ“‹ Contents
- [Introduction](#-introduction)
- [Key Features](#-key-features)
- [Installation](#ï¸-installation)
- [Quick Start](#-quick-start)
- [Advanced Usage](#-advanced-usage)
- [Supported Datasets](#-supported-datasets)
- [Roadmap](#-roadmap)
- [News](#-news)

## ğŸ“ Introduction

Standard ADD evaluations often rely on datasets that unevenly represent different synthesizers and lack diversity in bona fide speech, leading to biased and less reliable Equal Error Rate (EER) measurements.

Our framework introduces bona fide cross-testing, incorporating multiple bona fide speech datasets to ensure balanced assessments. By aggregating EERs across diverse speech types, BonaFideCrossEval improves robustness, interpretability, and real-world applicability.

We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research. ğŸš€

<p align="center">
  <img src="docs/en/_static/images/eval_framework.png" width="100%">
  <br>Audio Deepfake Detection Eval Framework
</p>

## âœ¨ Key Features

1. **Spoof Cross Testing**: Evaluate models by pairing a single bona fide speech type with multiple synthesizer datasets
2. **Bona Fide Cross Testing**: Test against diverse bona fide speech types for robust evaluation
3. **EER Aggregation**: Summarize results using maximum pooling to identify challenging synthesizers
4. **Comprehensive Visualization**: Generate detailed plots and statistics for analysis
5. **Flexible Dataset Support**: Evaluate on standard and custom datasets

## ğŸ› ï¸ Installation

### Method 1: Using pip

1. Create and activate a virtual environment (recommended)
   ```bash
   # Create virtual environment
   python -m venv add_eval_env
   
   # Activate virtual environment
   # On Windows
   .\add_eval_env\Scripts\activate
   # On Unix or MacOS
   source add_eval_env/bin/activate
   ```

2. Install dependencies
   ```bash
   pip install -r add_detect_eval/requirements.txt
   ```

### Method 2: Manual Installation

```bash
git clone https://github.com/ntu-dtc/audio_deepfake_eval.git
cd audio_deepfake_eval
pip install -r add_detect_eval/requirements.txt
```

### Dependencies
- Python >= 3.8
- numpy
- scipy
- matplotlib
- pandas
- PyYAML
- scikit-learn

## ğŸš€ Quick Start

### 1. Prepare Configuration

Create `config.yaml`:
```yaml
model_name: "conformer-based-classifier-for-anti-spoofing"
eer_files_dir: "./eer_files"

datasets:
  - name: "asvspoof_2021_DF"
    include_patterns:
      - ".wav"
      - "p227"
      - "p228"
  - name: "academicodec_hifi_16k_320d"
    include_patterns:
```

The `include_patterns` field is optional for each dataset. When specified:
- Only audio samples whose IDs match any of the patterns will be included in the evaluation
- Patterns can be file extensions (e.g., ".wav"), speaker IDs (e.g., "p227"), or any other ID pattern
- If not specified, all samples in the dataset will be evaluated
- Multiple patterns can be specified to include different subsets of samples

For example, to evaluate only samples from speakers p227 and p228:
```yaml
datasets:
  - name: "vctk"
    include_patterns:
      - "p227"  # Include samples from speaker p227
      - "p228"  # Include samples from speaker p228
```

### 2. Run Evaluation

```bash
python add_detect_eval/compute_eers.py --config config.yaml --dataset asvspoof_2021_DF
```

If you don't specify a dataset, the script will show available datasets:
```
ERROR: Please specify a dataset to evaluate using --dataset

Available datasets in config:
   asvspoof_2021_DF
   academicodec_hifi_16k_320d

Example usage:
   python add_detect_eval/compute_eers.py --config config.yaml --dataset <dataset_name>
```

Example output:
```
Processing dataset: [asvspoof_2021_DF]
   Including patterns: *.wav

Computing EER...
EER: 3.47%

Codec statistics:
   mp3: 500 files
   aac: 300 files
   opus: 200 files

Synthesizer statistics:
   fastspeech2: 400 files
   tacotron2: 300 files
   glow-tts: 300 files
```

### 3. Visualize Results

```bash
python add_detect_eval/visualize_stats.py \
  --config config.yaml \
  --dataset asvspoof_2021_DF \
  --output-dir ./visualizations
```

If you don't specify a dataset, the script will show available datasets:
```
ERROR: Please specify a dataset to visualize using --dataset

Available datasets in config:
   asvspoof_2021_DF
   academicodec_hifi_16k_320d

Example usage:
   python add_detect_eval/visualize_stats.py --config config.yaml --dataset <dataset_name>
```

Example output:
```
Processing dataset: [asvspoof_2021_DF]
   Including patterns: *.wav

Visualization saved to ./visualizations/asvspoof_2021_DF_statistics.png
```

## ğŸ” Advanced Usage

### Cross-Testing Evaluation

Evaluate models across different combinations of bonafide and spoof datasets:

```bash
python add_detect_eval/compute_eers_cross_testing.py \
  --config config.yaml \
  --output-dir ./output \
  --plot-dir ./visualizations
```

#### Example Output Structure

```
output/
â”œâ”€â”€ eer_matrix.csv         # Cross-testing EER results in CSV format
â”œâ”€â”€ eer_details.json       # Detailed EER calculations and thresholds
â””â”€â”€ results.json          # Summary statistics and metadata

visualizations/
â””â”€â”€ cross_testing_matrix.png  # Heatmap visualization of EER matrix
```

#### Example Log Output

```
Processing dataset: [asvspoof_2021_DF]
   Including patterns: *.wav

Dataset Summary:
   Bonafide subsets: [librispeech_clean], [vctk]
   Spoof subsets: [asvspoof_2021_DF], [av_deepfake_1m]

Computing EER:
   Bonafide subset:  [vctk]
   Spoof subset:     [av_deepfake_1m]
   Using synthesizer: [vits_word]
Sample counts:
   Spoof samples:    180
   Bonafide samples: 755
EER: 3.47%

Results saved successfully:
   EER results and statistics: ./output
   Visualization plots: ./visualizations
```

#### Example Results

1. **EER Matrix (eer_matrix.csv)**:
```csv
Dataset,librispeech_clean,librispeech_other,vctk
fastspeech2,0.0347,0.0412,0.0389
tacotron2,0.0523,0.0678,0.0456
glow-tts,0.0434,0.0567,0.0412
```

2. **Detailed Results (eer_details.json)**:
```json
{
  "librispeech_clean": {
    "asvspoof_2021_fastspeech2": {
      "eer": 0.0347,
      "threshold": -3.621,
      "spoof_subset": "asvspoof_2021_DF",
      "synthesizer": "fastspeech2"
    }
  }
}
```

3. **Cross-Testing Matrix Visualization**:

<p align="center">
  <img src="docs/en/_static/images/cross_testing_matrix.png" width="50%">
  <br>
  <em>Cross-Testing EER Matrix Heatmap</em>
</p>

The heatmap shows EER values (%) for each combination of:
- X-axis: Bonafide speech datasets
- Y-axis: Synthesizer IDs
- Color intensity: EER value (darker = higher EER)

For detailed documentation, see our [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html).

## ğŸ“Š Supported Datasets

### Audio Codecs
âœ… CodecFake  
âœ… academicodec_hifi_16k_320d  
âœ… audiodec_24k_320d  
âœ… descript-audio-codec-16khz  
âœ… encodec_24khz  
âœ… funcodec-funcodec_en_libritts-16k-nq32ds320  

### Speech Datasets  
âœ… librispeech_test_clean  
âœ… librispeech_test_other  
âœ… ami_ihm  
âœ… ami_sdm  
âœ… vctk  

### Deepfake Detection Datasets
âœ… asvspoof_2021_DF  
âœ… asvspoof2019_la  
âœ… av_deepfake_1m  
âœ… emofake  
âœ… fakeavceleb  
âœ… llamapartialspoof_r01tts0a  
âœ… llamapartialspoof_r01tts0b  
âœ… mlaad  
âœ… partialspoof  
âœ… release_in_the_wild  
âœ… scenefake  
âœ… speech_tokenizer  

## ğŸ“ˆ Leaderboard

<p align="center">
  <img src="docs/en/_static/images/leaderboard.jpeg" width="100%">
  <br>
  <em>Model Performance Comparison on Different Datasets</em>
</p>

The leaderboard shows Equal Error Rate (EER) performance of different models across various datasets. Lower EER indicates better performance in distinguishing between genuine and spoofed audio.

## ğŸ”œ Roadmap

### Completed
âœ… Single dataset EER evaluation  
âœ… Compute audio statistics for datasets  
âœ… Audio statistics visualization  
âœ… Cross testing EER evaluation  
âœ… Cross testing EER visualization  

### In Progress
â¬œ Latex table results generation  
â¬œ Support for new audio codecs  
â¬œ Integration with more deepfake detection models  
â¬œ Real-time evaluation pipeline  

## ğŸ‰ News

- ğŸ”¥ **[2025.02.28]** Official release with benchmark results for 164 synthesizers and 9 bona fide speech styles
- ğŸ”¥ **[2024.09.18]** Documentation released with technical research and discussions. [ğŸ“– Read more](https://evalscope.readthedocs.io/en/refact_readme/blog/index.html)

> â­ If you like this project, please star it! Your support motivates us to keep improving.
