{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4f8007-bcd2-471c-9187-112d9531c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2e15cf-363f-470c-88d9-b73d12258784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbbce8b-2f67-4fbf-bd39-878308873aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trial_file(trial_file_path):\n",
    "    \"\"\"Read audio IDs from a trial file.\"\"\"\n",
    "    audio_ids = set()\n",
    "    try:\n",
    "        with open(trial_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Extract the audio ID (first column)\n",
    "                    audio_id = line.split()[0]\n",
    "                    # Remove file extension if present\n",
    "                    audio_id = os.path.splitext(audio_id)[0]\n",
    "                    audio_ids.add(audio_id)\n",
    "        return audio_ids\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading trial file {trial_file_path}: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb919213-f0c4-4eeb-9abe-c1e8a9282cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_score_file(score_file_path, audio_ids, output_dir):\n",
    "    \"\"\"Filter a score file to only include lines with audio IDs in the dataset.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Read the score file and filter lines\n",
    "        filtered_lines = []\n",
    "        with open(score_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    filtered_lines.append(line)\n",
    "                    continue\n",
    "                \n",
    "                # Extract the audio ID (first column)\n",
    "                parts = line.split()\n",
    "                if len(parts) < 1:\n",
    "                    continue\n",
    "                \n",
    "                audio_id = parts[0]\n",
    "                # Remove file extension if present\n",
    "                audio_id = os.path.splitext(audio_id)[0]\n",
    "                \n",
    "                # Check if this audio ID is in our dataset\n",
    "                if audio_id in audio_ids:\n",
    "                    filtered_lines.append(line)\n",
    "        \n",
    "        # Write the filtered lines to the output file\n",
    "        output_file = os.path.join(output_dir, os.path.basename(score_file_path))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write('\\n'.join(filtered_lines))\n",
    "        \n",
    "        return len(filtered_lines)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error filtering score file {score_file_path}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a9fe801-84bb-46fa-a6fb-68b102e597ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_key_file(key_file_path, audio_ids, output_dir):\n",
    "    \"\"\"Filter a key file to only include lines with audio IDs in the dataset.\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Read the key file and filter lines\n",
    "        filtered_lines = []\n",
    "        with open(key_file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    filtered_lines.append(line)\n",
    "                    continue\n",
    "                \n",
    "                # Extract the audio ID\n",
    "                parts = line.split()\n",
    "                if len(parts) < 1:\n",
    "                    continue\n",
    "                \n",
    "                audio_id = parts[1]\n",
    "                # Remove file extension if present\n",
    "                audio_id = os.path.splitext(audio_id)[0]\n",
    "                \n",
    "                # Check if this audio ID is in our dataset\n",
    "                if audio_id in audio_ids:\n",
    "                    filtered_lines.append(line)\n",
    "        \n",
    "        # Write the filtered lines to the output file\n",
    "        output_file = os.path.join(output_dir, os.path.basename(key_file_path))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write('\\n'.join(filtered_lines))\n",
    "        \n",
    "        return len(filtered_lines)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error filtering key file {key_file_path}: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1113a75-df2d-4be1-b6da-d8bd7ad02c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_dir, eer_files_dir, output_dir):\n",
    "    \"\"\"Process a single dataset.\"\"\"\n",
    "    dataset_name = os.path.basename(dataset_dir)\n",
    "    trial_file = os.path.join(dataset_dir, f\"{dataset_name}.cm.eval.trl.txt\")\n",
    "    \n",
    "    if not os.path.exists(trial_file):\n",
    "        logger.warning(f\"Trial file not found: {trial_file}\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Processing dataset: {dataset_name}\")\n",
    "    audio_ids = read_trial_file(trial_file)\n",
    "    logger.info(f\"Found {len(audio_ids)} audio IDs in {dataset_name}\")\n",
    "    \n",
    "    # Process each model in eer_files\n",
    "    for model_dir in os.listdir(eer_files_dir):\n",
    "        model_path = os.path.join(eer_files_dir, model_dir)\n",
    "        if not os.path.isdir(model_path):\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"Processing model: {model_dir}\")\n",
    "        \n",
    "        # Process scores\n",
    "        scores_dir = os.path.join(model_path, \"scores\")\n",
    "        if os.path.exists(scores_dir):\n",
    "            output_scores_dir = os.path.join(output_dir, model_dir, \"scores\")\n",
    "            # Create the output directory for scores\n",
    "            os.makedirs(output_scores_dir, exist_ok=True)\n",
    "            \n",
    "            for score_file in os.listdir(scores_dir):\n",
    "                if score_file.endswith(\".txt\") and dataset_name in score_file:\n",
    "                    score_path = os.path.join(scores_dir, score_file)\n",
    "                    filtered_count = filter_score_file(score_path, audio_ids, output_scores_dir)\n",
    "                    logger.info(f\"Filtered {filtered_count} lines in {score_file}\")\n",
    "        \n",
    "        # Process keys\n",
    "        keys_dir = os.path.join(model_path, \"keys\")\n",
    "        if os.path.exists(keys_dir):\n",
    "            output_keys_dir = os.path.join(output_dir, model_dir, \"keys\")\n",
    "            # Create the output directory for keys\n",
    "            os.makedirs(output_keys_dir, exist_ok=True)\n",
    "            \n",
    "            for key_file in os.listdir(keys_dir):\n",
    "                if key_file.endswith(\".txt\") and dataset_name in key_file:\n",
    "                    key_path = os.path.join(keys_dir, key_file)\n",
    "                    filtered_count = filter_key_file(key_path, audio_ids, output_keys_dir)\n",
    "                    logger.info(f\"Filtered {filtered_count} lines in {key_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da70b12-75cf-4f01-abea-d0a6b8b68295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 12:50:56 - INFO - Processing dataset: ami_ihm\n",
      "2025-04-03 12:50:56 - INFO - Found 600 audio IDs in ami_ihm\n",
      "2025-04-03 12:50:56 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_ihm.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_ihm.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_ihm.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_ihm.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_ihm.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_ihm.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing dataset: ami_sdm\n",
      "2025-04-03 12:50:56 - INFO - Found 600 audio IDs in ami_sdm\n",
      "2025-04-03 12:50:56 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_sdm.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_sdm.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_sdm.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_sdm.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_sdm.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 600 lines in ami_sdm.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing dataset: asvspoof2019_la\n",
      "2025-04-03 12:50:56 - INFO - Found 8400 audio IDs in asvspoof2019_la\n",
      "2025-04-03 12:50:56 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:50:56 - INFO - Filtered 8400 lines in asvspoof2019_la.txt\n",
      "2025-04-03 12:50:56 - INFO - Filtered 8400 lines in asvspoof2019_la.txt\n",
      "2025-04-03 12:50:56 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:50:56 - INFO - Filtered 8400 lines in asvspoof2019_la.txt\n",
      "2025-04-03 12:50:57 - INFO - Filtered 8400 lines in asvspoof2019_la.txt\n",
      "2025-04-03 12:50:57 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:50:57 - INFO - Filtered 8400 lines in asvspoof2019_la.txt\n",
      "2025-04-03 12:50:57 - INFO - Filtered 8400 lines in asvspoof2019_la.txt\n",
      "2025-04-03 12:50:57 - INFO - Processing dataset: asvspoof2021_df\n",
      "2025-04-03 12:50:57 - INFO - Found 66000 audio IDs in asvspoof2021_df\n",
      "2025-04-03 12:50:57 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:50:57 - INFO - Filtered 66000 lines in asvspoof2021_df.txt\n",
      "2025-04-03 12:50:59 - INFO - Filtered 66000 lines in asvspoof2021_df.txt\n",
      "2025-04-03 12:50:59 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:50:59 - INFO - Filtered 66000 lines in asvspoof2021_df.txt\n",
      "2025-04-03 12:51:00 - INFO - Filtered 66000 lines in asvspoof2021_df.txt\n",
      "2025-04-03 12:51:00 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:01 - INFO - Filtered 66000 lines in asvspoof2021_df.txt\n",
      "2025-04-03 12:51:02 - INFO - Filtered 66000 lines in asvspoof2021_df.txt\n",
      "2025-04-03 12:51:02 - INFO - Processing dataset: emofake\n",
      "2025-04-03 12:51:02 - INFO - Found 3600 audio IDs in emofake\n",
      "2025-04-03 12:51:02 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:51:02 - INFO - Filtered 3600 lines in emofake.txt\n",
      "2025-04-03 12:51:02 - INFO - Filtered 3600 lines in emofake.txt\n",
      "2025-04-03 12:51:02 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:51:02 - INFO - Filtered 3600 lines in emofake.txt\n",
      "2025-04-03 12:51:02 - INFO - Filtered 3600 lines in emofake.txt\n",
      "2025-04-03 12:51:02 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:02 - INFO - Filtered 3600 lines in emofake.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 3600 lines in emofake.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing dataset: librispeech_test_clean\n",
      "2025-04-03 12:51:03 - INFO - Found 600 audio IDs in librispeech_test_clean\n",
      "2025-04-03 12:51:03 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_clean.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_clean.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_clean.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_clean.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_clean.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_clean.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing dataset: librispeech_test_other\n",
      "2025-04-03 12:51:03 - INFO - Found 600 audio IDs in librispeech_test_other\n",
      "2025-04-03 12:51:03 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_other.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_other.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_other.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_other.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_other.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 600 lines in librispeech_test_other.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing dataset: llamapartialspoof_r01tts0a\n",
      "2025-04-03 12:51:03 - INFO - Found 4200 audio IDs in llamapartialspoof_r01tts0a\n",
      "2025-04-03 12:51:03 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:51:03 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0a.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 4200 lines in llamapartialspoof_r01tts0a.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:51:03 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0a.txt\n",
      "2025-04-03 12:51:03 - INFO - Filtered 4200 lines in llamapartialspoof_r01tts0a.txt\n",
      "2025-04-03 12:51:03 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:04 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0a.txt\n",
      "2025-04-03 12:51:04 - INFO - Filtered 4200 lines in llamapartialspoof_r01tts0a.txt\n",
      "2025-04-03 12:51:04 - INFO - Processing dataset: llamapartialspoof_r01tts0b\n",
      "2025-04-03 12:51:04 - INFO - Found 3600 audio IDs in llamapartialspoof_r01tts0b\n",
      "2025-04-03 12:51:04 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:51:04 - INFO - Filtered 3602 lines in llamapartialspoof_r01tts0b.txt\n",
      "2025-04-03 12:51:04 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0b.txt\n",
      "2025-04-03 12:51:04 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:51:04 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0b.txt\n",
      "2025-04-03 12:51:05 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0b.txt\n",
      "2025-04-03 12:51:05 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:05 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0b.txt\n",
      "2025-04-03 12:51:05 - INFO - Filtered 3600 lines in llamapartialspoof_r01tts0b.txt\n",
      "2025-04-03 12:51:05 - INFO - Processing dataset: vctk\n",
      "2025-04-03 12:51:05 - INFO - Found 600 audio IDs in vctk\n",
      "2025-04-03 12:51:05 - INFO - Processing model: conformer-based-classifier-for-anti-spoofing\n",
      "2025-04-03 12:51:05 - INFO - Filtered 600 lines in vctk.txt\n",
      "2025-04-03 12:51:05 - INFO - Filtered 600 lines in vctk.txt\n",
      "2025-04-03 12:51:05 - INFO - Processing model: SCL-Deepfake-audio-detection\n",
      "2025-04-03 12:51:05 - INFO - Filtered 600 lines in vctk.txt\n",
      "2025-04-03 12:51:05 - INFO - Filtered 600 lines in vctk.txt\n",
      "2025-04-03 12:51:05 - INFO - Processing model: tcm_add\n",
      "2025-04-03 12:51:05 - INFO - Filtered 600 lines in vctk.txt\n",
      "2025-04-03 12:51:05 - INFO - Filtered 600 lines in vctk.txt\n",
      "2025-04-03 12:51:05 - INFO - Filtering complete. Filtered eer_files saved to eer_files_600\n"
     ]
    }
   ],
   "source": [
    "class MyArg:\n",
    "  def __init__(self, output_dir, datasets_dir, eer_files_dir):\n",
    "    self.output_dir = output_dir\n",
    "    self.datasets_dir = datasets_dir\n",
    "    self.eer_files_dir = eer_files_dir\n",
    "args = MyArg(\"eer_files_600\", \"datasets\", \"eer_files\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_dir in os.listdir(args.datasets_dir):\n",
    "    dataset_path = os.path.join(args.datasets_dir, dataset_dir)\n",
    "    if os.path.isdir(dataset_path):\n",
    "        process_dataset(dataset_path, args.eer_files_dir, args.output_dir)\n",
    "\n",
    "logger.info(f\"Filtering complete. Filtered eer_files saved to {args.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12881c1c-a0da-46c8-9205-b2584c323469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
